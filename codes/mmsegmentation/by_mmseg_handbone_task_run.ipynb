{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63a83355",
   "metadata": {},
   "source": [
    "# 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e8c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a2d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfce8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose\n",
    "from mmengine.runner import Runner, load_checkpoint\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmengine.logging import MMLogger, print_log\n",
    "from mmengine.structures import PixelData\n",
    "\n",
    "\n",
    "from mmseg.registry import DATASETS, TRANSFORMS, MODELS, METRICS\n",
    "from mmseg.datasets import BaseSegDataset, RandomRotate, BioMedicalGaussianBlur, CLAHE, RandomCutOut\n",
    "from mmseg.models.segmentors import EncoderDecoder\n",
    "from mmseg.models.decode_heads import ASPPHead, FCNHead, SegformerHead\n",
    "from mmseg.models.utils.wrappers import resize\n",
    "\n",
    "\n",
    "from mmcv.transforms import BaseTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11cb831",
   "metadata": {},
   "source": [
    "# 1. Init Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d172c1-c1d7-427f-aa04-047cd054512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed fix\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "seed = 21\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bdfd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 입력\n",
    "\n",
    "IMAGE_ROOT = ${train_data_dir}\n",
    "LABEL_ROOT = ${train_label_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10ec100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Class\n",
    "\n",
    "CLASSES = [\n",
    "    'finger-1', 'finger-2', 'finger-3', 'finger-4', 'finger-5',\n",
    "    'finger-6', 'finger-7', 'finger-8', 'finger-9', 'finger-10',\n",
    "    'finger-11', 'finger-12', 'finger-13', 'finger-14', 'finger-15',\n",
    "    'finger-16', 'finger-17', 'finger-18', 'finger-19', 'Trapezium',\n",
    "    'Trapezoid', 'Capitate', 'Hamate', 'Scaphoid', 'Lunate',\n",
    "    'Triquetrum', 'Pisiform', 'Radius', 'Ulna',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808d4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS2IND = {v: i for i, v in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f20766",
   "metadata": {},
   "outputs": [],
   "source": [
    "IND2CLASS = {v: k for k, v in CLASS2IND.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41dcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=IMAGE_ROOT)\n",
    "    for root, _dirs, files in os.walk(IMAGE_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".png\"\n",
    "}\n",
    "\n",
    "jsons = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=LABEL_ROOT)\n",
    "    for root, _dirs, files in os.walk(LABEL_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8148c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = sorted(pngs)\n",
    "jsons = sorted(jsons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b57297cf",
   "metadata": {},
   "source": [
    "# 2. MixIn Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c83fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessResultMixin:\n",
    "    def postprocess_result(self,\n",
    "                           seg_logits,\n",
    "                           data_samples):\n",
    "        \"\"\" Convert results list to `SegDataSample`.\n",
    "        Args:\n",
    "            seg_logits (Tensor): The segmentation results, seg_logits from\n",
    "                model of each input image.\n",
    "            data_samples (list[:obj:`SegDataSample`]): The seg data samples.\n",
    "                It usually includes information such as `metainfo` and\n",
    "                `gt_sem_seg`. Default to None.\n",
    "        Returns:\n",
    "            list[:obj:`SegDataSample`]: Segmentation results of the\n",
    "            input images. Each SegDataSample usually contain:\n",
    "\n",
    "            - ``pred_sem_seg``(PixelData): Prediction of semantic segmentation.\n",
    "            - ``seg_logits``(PixelData): Predicted logits of semantic\n",
    "                segmentation before normalization.\n",
    "        \"\"\"\n",
    "        batch_size, C, H, W = seg_logits.shape\n",
    "\n",
    "        if data_samples is None:\n",
    "            data_samples = [SegDataSample() for _ in range(batch_size)]\n",
    "            only_prediction = True\n",
    "        else:\n",
    "            only_prediction = False\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if not only_prediction:\n",
    "                img_meta = data_samples[i].metainfo\n",
    "                # remove padding area\n",
    "                if 'img_padding_size' not in img_meta:\n",
    "                    padding_size = img_meta.get('padding_size', [0] * 4)\n",
    "                else:\n",
    "                    padding_size = img_meta['img_padding_size']\n",
    "                padding_left, padding_right, padding_top, padding_bottom =\\\n",
    "                    padding_size\n",
    "                # i_seg_logits shape is 1, C, H, W after remove padding\n",
    "                i_seg_logits = seg_logits[i:i + 1, :,\n",
    "                                          padding_top:H - padding_bottom,\n",
    "                                          padding_left:W - padding_right]\n",
    "\n",
    "                flip = img_meta.get('flip', None)\n",
    "                if flip:\n",
    "                    flip_direction = img_meta.get('flip_direction', None)\n",
    "                    assert flip_direction in ['horizontal', 'vertical']\n",
    "                    if flip_direction == 'horizontal':\n",
    "                        i_seg_logits = i_seg_logits.flip(dims=(3, ))\n",
    "                    else:\n",
    "                        i_seg_logits = i_seg_logits.flip(dims=(2, ))\n",
    "\n",
    "                # resize as original shape\n",
    "                i_seg_logits = resize(\n",
    "                    i_seg_logits,\n",
    "                    size=img_meta['ori_shape'],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners,\n",
    "                    warning=False).squeeze(0)\n",
    "            else:\n",
    "                i_seg_logits = seg_logits[i]\n",
    "\n",
    "            i_seg_logits = i_seg_logits.sigmoid()\n",
    "            i_seg_pred = (i_seg_logits > 0.5).to(i_seg_logits)\n",
    "                \n",
    "            data_samples[i].set_data({\n",
    "                'seg_logits':\n",
    "                PixelData(**{'data': i_seg_logits}),\n",
    "                'pred_sem_seg':\n",
    "                PixelData(**{'data': i_seg_pred})\n",
    "            })\n",
    "\n",
    "        return data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9e3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class EncoderDecoderWithoutArgmax(PostProcessResultMixin, EncoderDecoder):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a42dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossByFeatMixIn:\n",
    "    def loss_by_feat(self, seg_logits, batch_data_samples) -> dict:\n",
    "        \"\"\"Compute segmentation loss.\n",
    "\n",
    "        Args:\n",
    "            seg_logits (Tensor): The output from decode head forward function.\n",
    "            batch_data_samples (List[:obj:`SegDataSample`]): The seg\n",
    "                data samples. It usually includes information such\n",
    "                as `metainfo` and `gt_sem_seg`.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Tensor]: a dictionary of loss components\n",
    "        \"\"\"\n",
    "\n",
    "        seg_label = self._stack_batch_gt(batch_data_samples)\n",
    "        loss = dict()\n",
    "        seg_logits = resize(\n",
    "            input=seg_logits,\n",
    "            size=seg_label.shape[2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=self.align_corners)\n",
    "        if self.sampler is not None:\n",
    "            seg_weight = self.sampler.sample(seg_logits, seg_label)\n",
    "        else:\n",
    "            seg_weight = None\n",
    "\n",
    "        if not isinstance(self.loss_decode, nn.ModuleList):\n",
    "            losses_decode = [self.loss_decode]\n",
    "        else:\n",
    "            losses_decode = self.loss_decode\n",
    "        for loss_decode in losses_decode:\n",
    "            if loss_decode.loss_name not in loss:\n",
    "                loss[loss_decode.loss_name] = loss_decode(\n",
    "                    seg_logits,\n",
    "                    seg_label,\n",
    "                    weight=seg_weight,\n",
    "                    ignore_index=self.ignore_index)\n",
    "            else:\n",
    "                loss[loss_decode.loss_name] += loss_decode(\n",
    "                    seg_logits,\n",
    "                    seg_label,\n",
    "                    weight=seg_weight,\n",
    "                    ignore_index=self.ignore_index)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0184d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class ASPPHeadWithoutAccuracy(LossByFeatMixIn, ASPPHead):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c289137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class FCNHeadWithoutAccuracy(LossByFeatMixIn, FCNHead):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c727ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class SegformerHeadWithoutAccuracy(LossByFeatMixIn, SegformerHead):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdd52579",
   "metadata": {},
   "source": [
    "# 3. Dataset Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6c20f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DATASETS.register_module()\n",
    "class XRayDataset(BaseSegDataset):\n",
    "    def __init__(self, is_train, **kwargs):\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def load_data_list(self):\n",
    "        _filenames = np.array(pngs)\n",
    "        _labelnames = np.array(jsons)\n",
    "        \n",
    "        # split train-valid\n",
    "        # 동일 인물의 손이 train, valid로 분리되는 것을 예방\n",
    "        groups = [os.path.dirname(fname) for fname in _filenames]\n",
    "        \n",
    "        # dummy label\n",
    "        ys = [0 for fname in _filenames]\n",
    "        \n",
    "        # 전체 데이터의 20%를 validation data로 split\n",
    "        gkf = GroupKFold(n_splits=5)\n",
    "        \n",
    "        filenames = []\n",
    "        labelnames = []\n",
    "        for i, (x, y) in enumerate(gkf.split(_filenames, ys, groups)):\n",
    "            if self.is_train:\n",
    "                # 0번을 validation dataset으로 사용\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                filenames += list(_filenames[y])\n",
    "                labelnames += list(_labelnames[y])\n",
    "            else:\n",
    "                filenames = list(_filenames[y])\n",
    "                labelnames = list(_labelnames[y])\n",
    "                # skip i > 0\n",
    "                break\n",
    "        \n",
    "        data_list = []\n",
    "        for i, (img_path, ann_path) in enumerate(zip(filenames, labelnames)):\n",
    "            data_info = dict(\n",
    "                img_path=os.path.join(IMAGE_ROOT, img_path),\n",
    "                seg_map_path=os.path.join(LABEL_ROOT, ann_path),\n",
    "            )\n",
    "            data_list.append(data_info)\n",
    "        \n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc590843",
   "metadata": {},
   "outputs": [],
   "source": [
    "@TRANSFORMS.register_module()\n",
    "class LoadXRayAnnotations(BaseTransform):\n",
    "    def transform(self, result):\n",
    "        label_path = result[\"seg_map_path\"]\n",
    "        \n",
    "        image_size = (2048, 2048)\n",
    "        \n",
    "        # process a label of shape (H, W, NC)\n",
    "        label_shape = image_size + (len(CLASSES), )\n",
    "        label = np.zeros(label_shape, dtype=np.uint8)\n",
    "        \n",
    "        # read label file\n",
    "        with open(label_path, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "        annotations = annotations[\"annotations\"]\n",
    "        \n",
    "        # iterate each class\n",
    "        for ann in annotations:\n",
    "            c = ann[\"label\"]\n",
    "            class_ind = CLASS2IND[c]\n",
    "            points = np.array(ann[\"points\"])\n",
    "            \n",
    "            # polygon to mask\n",
    "            class_label = np.zeros(image_size, dtype=np.uint8)\n",
    "            cv2.fillPoly(class_label, [points], 1)\n",
    "            label[..., class_ind] = class_label\n",
    "        \n",
    "        result[\"gt_seg_map\"] = label\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ac2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@TRANSFORMS.register_module()\n",
    "class TransposeAnnotations(BaseTransform):\n",
    "    def transform(self, result):\n",
    "        result[\"gt_seg_map\"] = np.transpose(result[\"gt_seg_map\"], (2, 0, 1))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a4f47c5",
   "metadata": {},
   "source": [
    "# 4. Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d228a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@METRICS.register_module()\n",
    "class DiceMetric(BaseMetric):\n",
    "    def __init__(self,\n",
    "                 collect_device='cpu',\n",
    "                 prefix=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "        \n",
    "    @staticmethod\n",
    "    def dice_coef(y_true, y_pred):        \n",
    "        y_true_f = y_true.flatten(-2)\n",
    "        y_pred_f = y_pred.flatten(-2)\n",
    "        intersection = torch.sum(y_true_f * y_pred_f, -1)\n",
    "\n",
    "        eps = 0.0001\n",
    "        return (2. * intersection + eps) / (torch.sum(y_true_f, -1) + torch.sum(y_pred_f, -1) + eps)\n",
    "\n",
    "    def process(self, data_batch, data_samples):\n",
    "        \"\"\"Process one batch of data and data_samples.\n",
    "\n",
    "        The processed results should be stored in ``self.results``, which will\n",
    "        be used to compute the metrics when all batches have been processed.\n",
    "\n",
    "        Args:\n",
    "            data_batch (dict): A batch of data from the dataloader.\n",
    "            data_samples (Sequence[dict]): A batch of outputs from the model.\n",
    "        \"\"\"\n",
    "        for data_sample in data_samples:\n",
    "            pred_label = data_sample['pred_sem_seg']['data']\n",
    "            \n",
    "            label = data_sample['gt_sem_seg']['data'].to(pred_label)\n",
    "            self.results.append(\n",
    "                self.dice_coef(label, pred_label)\n",
    "            )\n",
    "\n",
    "    def compute_metrics(self, results):\n",
    "        \"\"\"Compute the metrics from processed results.\n",
    "\n",
    "        Args:\n",
    "            results (list): The processed results of each batch.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: The computed metrics. The keys are the names of\n",
    "                the metrics, and the values are corresponding results.\n",
    "        \"\"\"\n",
    "        logger: MMLogger = MMLogger.get_current_instance()\n",
    "            \n",
    "        results = torch.stack(self.results, 0)        \n",
    "        dices_per_class = torch.mean(results, 0)\n",
    "        avg_dice = torch.mean(dices_per_class)\n",
    "            \n",
    "        ret_metrics = {\n",
    "            \"Dice\": dices_per_class.detach().cpu().numpy(),\n",
    "        }\n",
    "        # summary table\n",
    "        ret_metrics_summary = OrderedDict({\n",
    "            ret_metric: np.round(np.nanmean(ret_metric_value) * 100, 2)\n",
    "            for ret_metric, ret_metric_value in ret_metrics.items()\n",
    "        })\n",
    "        \n",
    "        metrics = {\n",
    "            \"mDice\": torch.mean(dices_per_class).item()\n",
    "        }\n",
    "\n",
    "        # each class table\n",
    "        ret_metrics.pop('aAcc', None)\n",
    "        ret_metrics_class = OrderedDict({\n",
    "            ret_metric: np.round(ret_metric_value * 100, 2)\n",
    "            for ret_metric, ret_metric_value in ret_metrics.items()\n",
    "        })\n",
    "        ret_metrics_class.update({'Class': CLASSES})\n",
    "        ret_metrics_class.move_to_end('Class', last=False)\n",
    "        class_table_data = PrettyTable()\n",
    "        for key, val in ret_metrics_class.items():\n",
    "            class_table_data.add_column(key, val)\n",
    "\n",
    "        print_log('per class results:', logger)\n",
    "        print_log('\\n' + class_table_data.get_string(), logger=logger)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1380e2d8",
   "metadata": {},
   "source": [
    "# 5. Configs Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17734392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset_setting.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset_setting.py\n",
    "\n",
    "# dataset settings\n",
    "dataset_type = 'XRayDataset'\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadXRayAnnotations'),\n",
    "    dict(type='Resize', scale=(1024, 1024)),\n",
    "    #dict(type='RandomRotate', prob=0.5, degree=90.),\n",
    "    #dict(type='BioMedicalGaussianBlur'),\n",
    "    dict(type='CLAHE', clip_limit=1.0, tile_grid_size=(8, 8)),\n",
    "    dict(type='RandomCutOut', prob=0.5, n_holes=(20, 40), cutout_ratio=(0.2, 0.2)),\n",
    "    dict(type='TransposeAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "val_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(1024, 1024)),\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='LoadXRayAnnotations'),\n",
    "    dict(type='TransposeAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "test_pipeline=[\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(1024, 1024)),\n",
    "    #dict(type='BioMedicalGaussianBlur'),\n",
    "    dict(type='CLAHE', clip_limit=1.0, tile_grid_size=(8, 8)),\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "train_dataloader = dict(\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='InfiniteSampler', shuffle=True),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        is_train=True,\n",
    "        pipeline=train_pipeline\n",
    "    )\n",
    ")\n",
    "val_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        is_train=False,\n",
    "        pipeline=val_pipeline\n",
    "    )\n",
    ")\n",
    "test_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        is_train=False,\n",
    "        pipeline=test_pipeline\n",
    "    )\n",
    ")\n",
    "\n",
    "val_evaluator = dict(type='DiceMetric')\n",
    "test_evaluator = val_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4455f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config_for_this_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config_for_this_example.py\n",
    "\n",
    "# Train Segformer Mit B0\n",
    "_base_ = [\n",
    "    #\"./configs/_base_/models/segformer_mit-b0.py\",\n",
    "    \"./configs/segformer/segformer_mit-b5_8xb2-160k_ade20k-512x512.py\",\n",
    "    \"dataset_setting.py\",\n",
    "    \"./configs/_base_/default_runtime.py\",\n",
    "    #\"./configs/_base_/schedules/schedule_160k.py\"\n",
    "]\n",
    "# mean=[123.675, 116.28, 103.53],\n",
    "#     std=[58.395, 57.12, 57.375],\n",
    "data_preprocessor = dict(\n",
    "    type='SegDataPreProcessor',\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375],\n",
    "    #mean=[0., 0., 0.],\n",
    "    #std=[255., 255., 255.],\n",
    "    bgr_to_rgb=True,\n",
    "    size=(1024, 1024),\n",
    "    pad_val=0,\n",
    "    seg_pad_val=255,\n",
    ")\n",
    "\n",
    "model = dict(\n",
    "    type='EncoderDecoderWithoutArgmax',\n",
    "    init_cfg=dict(\n",
    "        type='Pretrained',\n",
    "        # load ADE20k pretrained EncoderDecoder from mmsegmentation\n",
    "        checkpoint=\"https://download.openmmlab.com/mmsegmentation/v0.5/segformer/segformer_mit-b5_512x512_160k_ade20k/segformer_mit-b5_512x512_160k_ade20k_20210726_145235-94cedf59.pth\"\n",
    "        #checkpoint=\"https://download.openmmlab.com/mmsegmentation/v0.5/segformer/segformer_mit-b0_512x512_160k_ade20k/segformer_mit-b0_512x512_160k_ade20k_20210726_101530-8ffa8fda.pth\"\n",
    "    ),\n",
    "    data_preprocessor=data_preprocessor,\n",
    "    decode_head=dict(\n",
    "        type='SegformerHeadWithoutAccuracy',\n",
    "        num_classes=29,\n",
    "        loss_decode=dict(\n",
    "            type='CrossEntropyLoss',\n",
    "            use_sigmoid=True,\n",
    "            loss_weight=1.0,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "optimizer = dict(type='AdamW', lr=0.00006, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "#optim_wrapper = dict(type='OptimWrapper', optimizer=optimizer, clip_grad=None)\n",
    "optim_wrapper = dict(\n",
    "    type='OptimWrapper',\n",
    "    optimizer=dict(\n",
    "        type='AdamW', lr=0.00006, betas=(0.9, 0.999), weight_decay=0.01),\n",
    "    paramwise_cfg=dict(\n",
    "        custom_keys={\n",
    "            'pos_block': dict(decay_mult=0.),\n",
    "            'norm': dict(decay_mult=0.),\n",
    "            'head': dict(lr_mult=10.)\n",
    "        }))\n",
    "# learning policy\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='LinearLR', start_factor=1e-6, by_epoch=False, begin=0, end=1500\n",
    "    ),\n",
    "    dict(\n",
    "        type='PolyLR',\n",
    "        eta_min=0.0,\n",
    "        power=1.0,\n",
    "        begin=1500,\n",
    "        end=160000,\n",
    "        by_epoch=False,\n",
    "    )\n",
    "]\n",
    "# training schedule for 20k\n",
    "train_cfg = dict(type='IterBasedTrainLoop', max_iters=20000, val_interval=2000)\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "default_hooks = dict(\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "    logger=dict(type='LoggerHook', interval=50, log_metric_by_epoch=False),\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "    checkpoint=dict(type='CheckpointHook', by_epoch=False, interval=2000),\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "    visualization=dict(type='SegVisualizationHook')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903a66e1",
   "metadata": {},
   "source": [
    "# 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ea76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "cfg = Config.fromfile(\"config_for_this_example.py\")\n",
    "cfg.launcher = \"none\"\n",
    "cfg.work_dir = os.path.join('./work_dirs', \"segformer\")\n",
    "\n",
    "# resume training\n",
    "cfg.resume = False\n",
    "\n",
    "runner = Runner.from_cfg(cfg)\n",
    "\n",
    "# start training\n",
    "runner.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b2bf304",
   "metadata": {},
   "source": [
    "# 7. Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92658f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODELS.build(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint(\n",
    "    model,\n",
    "    \"work_dirs/segformer/iter_20000.pth\",\n",
    "    map_location='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprare_data(imgs, model):\n",
    "    for t in cfg.test_pipeline:\n",
    "        if t.get('type') in ['LoadXRayAnnotations', 'TransposeAnnotations']:\n",
    "            cfg.test_pipeline.remove(t)\n",
    "\n",
    "    is_batch = True\n",
    "    if not isinstance(imgs, (list, tuple)):\n",
    "        imgs = [imgs]\n",
    "        is_batch = False\n",
    "\n",
    "    if isinstance(imgs[0], np.ndarray):\n",
    "        cfg.test_pipeline[0]['type'] = 'LoadImageFromNDArray'\n",
    "\n",
    "    pipeline = Compose(cfg.test_pipeline)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    for img in imgs:\n",
    "        if isinstance(img, np.ndarray):\n",
    "            data_ = dict(img=img)\n",
    "        else:\n",
    "            data_ = dict(img_path=img)\n",
    "        data_ = pipeline(data_)\n",
    "        data['inputs'].append(data_['inputs'])\n",
    "        data['data_samples'].append(data_['data_samples'])\n",
    "\n",
    "    return data, is_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PALETTE = [\n",
    "    (220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228),\n",
    "    (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30),\n",
    "    (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42),\n",
    "    (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157),\n",
    "    (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240),\n",
    "    (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176),\n",
    "]\n",
    "\n",
    "def label2rgb(label):\n",
    "    image_size = label.shape[1:] + (3, )\n",
    "    image = np.zeros(image_size, dtype=np.uint8)\n",
    "    \n",
    "    for i, class_label in enumerate(label):\n",
    "        image[class_label == 1] = PALETTE[i]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(IMAGE_ROOT, \"ID001\", \"image.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "data, is_batch = _preprare_data(img, model)\n",
    "\n",
    "# forward the model\n",
    "with torch.no_grad():\n",
    "    results = model.test_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label2rgb(results[0].pred_sem_seg.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c89cf2-aee7-427f-b341-de94944dff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_rle_to_mask(rle, height, width):\n",
    "    s = rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(height * width, dtype=np.uint8)\n",
    "    \n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    \n",
    "    return img.reshape(height, width)\n",
    "\n",
    "def encode_mask_to_rle(mask):\n",
    "    '''\n",
    "    mask: numpy array binary mask \n",
    "    1 - mask \n",
    "    0 - background\n",
    "    Returns encoded run length \n",
    "    '''\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e82ccc-2f1a-4bc9-b2bf-8b73c75cd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "TEST_IMAGE_ROOT = ${test_img_dir}\n",
    "\n",
    "rles = []\n",
    "filename_and_class = []\n",
    "n_class = len(CLASSES)\n",
    "thr = 0.5\n",
    "\n",
    "for root, _dirs, files in sorted(os.walk(TEST_IMAGE_ROOT)):\n",
    "    for fname in files:\n",
    "        if os.path.splitext(fname)[1].lower() == '.png':\n",
    "            img = cv2.imread(os.path.join(root, fname))\n",
    "            data, is_batch = _preprare_data(img, model)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                results = model.test_step(data)\n",
    "                \n",
    "            outputs = results[0].pred_sem_seg.data\n",
    "            \n",
    "            \n",
    "            for c, segm in enumerate(outputs):\n",
    "                rle = encode_mask_to_rle(segm)\n",
    "                rles.append(rle)\n",
    "                filename_and_class.append(f\"{IND2CLASS[c]}_{root.split('/')[-1]}/{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20e4c3-2188-4d35-9f27-51e103f6d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classes, filename = zip(*[x.split(\"_\") for x in filename_and_class])\n",
    "image_name = [os.path.basename(f) for f in filename]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"image_name\": image_name,\n",
    "    \"class\": classes,\n",
    "    \"rle\": rles,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655bb255-614e-44d7-b155-ac171a7efcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
